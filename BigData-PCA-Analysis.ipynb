{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Principal Component Analysis (PCA) Implementation\n",
    "## CSE 4460 - Big Data Analytics Lab - Open Ended Experiment\n",
    "\n",
    "### Project Overview\n",
    "This notebook demonstrates a scalable PCA implementation using Apache Spark for high-dimensional datasets. We'll work with an expanded California Housing dataset containing millions of records with engineered features that create high dimensionality and redundancy.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand big data architecture for PCA\n",
    "- Implement distributed PCA using Apache Spark\n",
    "- Handle high-dimensional data with memory constraints\n",
    "- Validate and visualize PCA results\n",
    "- Analyze performance and scalability trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's install and import all necessary libraries for our big data PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pyspark==3.5.1\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler as SparkStandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.mllib.linalg import Vectors as MLLibVectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Set display options\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Big Data Architecture Setup\n",
    "\n",
    "### 2.1 Spark Session Configuration\n",
    "We'll configure Spark for optimal PCA performance with memory management and parallelization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark Session for PCA workload\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create optimized Spark session for PCA operations\n",
    "    - Increased driver memory for large matrices\n",
    "    - Optimized serialization for ML operations\n",
    "    - Configured for local cluster simulation\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BigData_PCA_Analysis\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce noise\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Create Spark session\n",
    "spark = create_spark_session()\n",
    "\n",
    "print(f\"✅ Spark Session Created!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Available Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Driver Memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Big Data Architecture Overview\n",
    "\n",
    "**Architecture Components:**\n",
    "1. **Data Storage Layer**: Distributed file system (simulated with local partitions)\n",
    "2. **Processing Layer**: Apache Spark with MLlib for distributed linear algebra\n",
    "3. **Memory Management**: Optimized for large matrix operations\n",
    "4. **Compute Distribution**: Parallel processing across multiple cores\n",
    "\n",
    "**PCA-Specific Challenges:**\n",
    "- **Memory Constraints**: Covariance matrix computation requires O(d²) memory\n",
    "- **Computational Complexity**: SVD computation is O(nd²) where n=samples, d=dimensions\n",
    "- **Data Distribution**: Need to minimize data shuffling during matrix operations\n",
    "- **Scalability**: Must handle datasets that don't fit in single-machine memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. High-Dimensional Dataset Creation\n",
    "\n",
    "We'll create a large, high-dimensional dataset based on California Housing data with engineered features that introduce redundancy and multicollinearity - perfect conditions for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_high_dimensional_dataset(n_samples=1000000, n_features_base=8, expansion_factor=15):\n",
    "    \"\"\"\n",
    "    Create a large, high-dimensional dataset with redundancy and multicollinearity\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Target number of samples (default: 1M)\n",
    "    - n_features_base: Base features from California housing\n",
    "    - expansion_factor: Factor to expand feature space\n",
    "    \n",
    "    Returns:\n",
    "    - pandas DataFrame with high-dimensional features\n",
    "    \"\"\"\n",
    "    print(\"🔄 Creating high-dimensional dataset...\")\n",
    "    \n",
    "    # Load base California housing data\n",
    "    california_housing = fetch_california_housing()\n",
    "    base_data = california_housing.data\n",
    "    feature_names = california_housing.feature_names\n",
    "    target = california_housing.target\n",
    "    \n",
    "    # Replicate data to reach target sample size\n",
    "    replications = n_samples // len(base_data) + 1\n",
    "    \n",
    "    # Replicate and add noise to create variations\n",
    "    expanded_data = []\n",
    "    expanded_targets = []\n",
    "    \n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # Add different noise patterns for each replication\n",
    "        noise_scale = 0.1 + (i * 0.05)  # Increasing noise with replications\n",
    "        noisy_data = base_data + np.random.normal(0, noise_scale, base_data.shape)\n",
    "        noisy_target = target + np.random.normal(0, 0.5, len(target))\n",
    "        \n",
    "        expanded_data.append(noisy_data)\n",
    "        expanded_targets.append(noisy_target)\n",
    "    \n",
    "    # Combine all replications\n",
    "    final_data = np.vstack(expanded_data)[:n_samples]\n",
    "    final_targets = np.hstack(expanded_targets)[:n_samples]\n",
    "    \n",
    "    # Create DataFrame with base features\n",
    "    df = pd.DataFrame(final_data, columns=feature_names)\n",
    "    df['target'] = final_targets\n",
    "    \n",
    "    # Add high-dimensional engineered features that create redundancy\n",
    "    print(\"🔄 Engineering high-dimensional features...\")\n",
    "    \n",
    "    # 1. Polynomial features (create multicollinearity)\n",
    "    df['MedInc_squared'] = df['MedInc'] ** 2\n",
    "    df['HouseAge_squared'] = df['HouseAge'] ** 2\n",
    "    df['AveRooms_squared'] = df['AveRooms'] ** 2\n",
    "    \n",
    "    # 2. Interaction features (more multicollinearity)\n",
    "    df['MedInc_x_HouseAge'] = df['MedInc'] * df['HouseAge']\n",
    "    df['AveRooms_x_AveBedrms'] = df['AveRooms'] * df['AveBedrms']\n",
    "    df['Population_x_AveOccup'] = df['Population'] * df['AveOccup']\n",
    "    \n",
    "    # 3. Logarithmic transformations\n",
    "    df['log_MedInc'] = np.log(df['MedInc'] + 1)\n",
    "    df['log_Population'] = np.log(df['Population'] + 1)\n",
    "    df['log_AveRooms'] = np.log(df['AveRooms'] + 1)\n",
    "    \n",
    "    # 4. Ratio features\n",
    "    df['Rooms_per_Household'] = df['AveRooms'] / (df['AveOccup'] + 0.001)\n",
    "    df['Bedrooms_per_Room'] = df['AveBedrms'] / (df['AveRooms'] + 0.001)\n",
    "    df['Population_density'] = df['Population'] / (df['AveOccup'] + 0.001)\n",
    "    \n",
    "    # 5. Binned features (categorical-like)\n",
    "    df['MedInc_bin_low'] = (df['MedInc'] < df['MedInc'].quantile(0.33)).astype(int)\n",
    "    df['MedInc_bin_mid'] = ((df['MedInc'] >= df['MedInc'].quantile(0.33)) & \n",
    "                           (df['MedInc'] < df['MedInc'].quantile(0.67))).astype(int)\n",
    "    df['MedInc_bin_high'] = (df['MedInc'] >= df['MedInc'].quantile(0.67)).astype(int)\n",
    "    \n",
    "    # 6. Geographic features (based on Latitude/Longitude)\n",
    "    df['Lat_rounded'] = np.round(df['Latitude'], 1)\n",
    "    df['Lon_rounded'] = np.round(df['Longitude'], 1)\n",
    "    df['Geographic_cluster'] = df['Lat_rounded'] * 100 + df['Lon_rounded']\n",
    "    \n",
    "    # 7. Time-based synthetic features (simulate time series)\n",
    "    time_periods = np.random.choice(range(1, 13), size=len(df))  # Months 1-12\n",
    "    df['Month'] = time_periods\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    \n",
    "    # 8. Add more synthetic correlated features\n",
    "    for i in range(expansion_factor):\n",
    "        # Create features that are linear combinations of existing features\n",
    "        weights = np.random.normal(0, 0.5, len(feature_names))\n",
    "        synthetic_feature = np.dot(final_data, weights) + np.random.normal(0, 0.1, n_samples)\n",
    "        df[f'synthetic_feature_{i}'] = synthetic_feature\n",
    "        \n",
    "        # Add noisy versions of existing features\n",
    "        base_feature = np.random.choice(feature_names)\n",
    "        df[f'noisy_{base_feature}_{i}'] = (df[base_feature] + \n",
    "                                          np.random.normal(0, 0.2 * df[base_feature].std(), n_samples))\n",
    "    \n",
    "    print(f\"✅ Dataset created with {len(df)} samples and {len(df.columns)} features\")\n",
    "    print(f\"📊 Dataset size: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the high-dimensional dataset\n",
    "start_time = time.time()\n",
    "df_large = create_high_dimensional_dataset(n_samples=500000, expansion_factor=10)  # 500K samples for demo\n",
    "creation_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️  Dataset creation time: {creation_time:.2f} seconds\")\n",
    "print(f\"📈 Final dataset shape: {df_large.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the created dataset\n",
    "print(\"📊 Dataset Overview:\")\n",
    "print(f\"Shape: {df_large.shape}\")\n",
    "print(f\"Memory usage: {df_large.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Data types: {df_large.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Check for multicollinearity (correlation matrix sample)\n",
    "print(\"\\n🔍 Checking for Multicollinearity (first 15 features):\")\n",
    "correlation_matrix = df_large.iloc[:, :15].corr()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"Found {len(high_corr_pairs)} highly correlated feature pairs (|r| > 0.7)\")\n",
    "for pair in high_corr_pairs[:5]:  # Show first 5\n",
    "    print(f\"  {pair[0]} ↔ {pair[1]}: r = {pair[2]:.3f}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n📈 Basic Statistics:\")\n",
    "print(df_large.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading into Spark\n",
    "\n",
    "Now we'll load our high-dimensional dataset into Spark DataFrame for distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_spark(pandas_df, spark_session, partition_count=None):\n",
    "    \"\"\"\n",
    "    Load pandas DataFrame to Spark with optimal partitioning\n",
    "    \n",
    "    Parameters:\n",
    "    - pandas_df: Input pandas DataFrame\n",
    "    - spark_session: Active Spark session\n",
    "    - partition_count: Number of partitions (auto if None)\n",
    "    \n",
    "    Returns:\n",
    "    - Spark DataFrame\n",
    "    \"\"\"\n",
    "    print(\"🔄 Loading data into Spark DataFrame...\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    spark_df = spark_session.createDataFrame(pandas_df)\n",
    "    \n",
    "    # Optimize partitioning for PCA operations\n",
    "    if partition_count is None:\n",
    "        # Rule of thumb: aim for 128MB per partition\n",
    "        data_size_mb = pandas_df.memory_usage(deep=True).sum() / 1024**2\n",
    "        partition_count = max(int(data_size_mb / 128), spark_session.sparkContext.defaultParallelism)\n",
    "    \n",
    "    spark_df = spark_df.repartition(partition_count)\n",
    "    \n",
    "    # Cache for multiple operations\n",
    "    spark_df.cache()\n",
    "    \n",
    "    # Trigger caching\n",
    "    row_count = spark_df.count()\n",
    "    \n",
    "    print(f\"✅ Data loaded into Spark DataFrame\")\n",
    "    print(f\"📊 Rows: {row_count:,}\")\n",
    "    print(f\"📊 Columns: {len(spark_df.columns)}\")\n",
    "    print(f\"🔧 Partitions: {spark_df.rdd.getNumPartitions()}\")\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "# Load data to Spark\n",
    "start_time = time.time()\n",
    "spark_df = load_data_to_spark(df_large, spark)\n",
    "loading_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️  Data loading time: {loading_time:.2f} seconds\")\n",
    "\n",
    "# Show schema\n",
    "print(\"\\n📋 Spark DataFrame Schema:\")\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing Pipeline\n",
    "\n",
    "Before applying PCA, we need to preprocess the data:\n",
    "1. Handle missing values\n",
    "2. Feature selection\n",
    "3. Feature scaling\n",
    "4. Vector assembly for MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_pca(spark_df, target_column='target'):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for PCA\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove target column and non-numeric features\n",
    "    2. Handle missing values\n",
    "    3. Feature selection (remove low-variance features)\n",
    "    4. Assemble features into vector\n",
    "    5. Scale features\n",
    "    \n",
    "    Parameters:\n",
    "    - spark_df: Input Spark DataFrame\n",
    "    - target_column: Name of target column to exclude\n",
    "    \n",
    "    Returns:\n",
    "    - processed_df: DataFrame ready for PCA\n",
    "    - feature_columns: List of feature column names\n",
    "    - scaler_model: Fitted StandardScaler model\n",
    "    \"\"\"\n",
    "    print(\"🔄 Starting data preprocessing pipeline...\")\n",
    "    \n",
    "    # 1. Select numeric features (exclude target)\n",
    "    numeric_columns = []\n",
    "    for field in spark_df.schema.fields:\n",
    "        if (field.dataType in [DoubleType(), FloatType(), IntegerType(), LongType()] and \n",
    "            field.name != target_column):\n",
    "            numeric_columns.append(field.name)\n",
    "    \n",
    "    print(f\"📊 Selected {len(numeric_columns)} numeric features for PCA\")\n",
    "    \n",
    "    # 2. Select only numeric columns\n",
    "    df_numeric = spark_df.select(numeric_columns)\n",
    "    \n",
    "    # 3. Handle missing values (fill with column mean)\n",
    "    print(\"🔧 Handling missing values...\")\n",
    "    \n",
    "    # Calculate means for missing value imputation\n",
    "    means = {}\n",
    "    for col in numeric_columns:\n",
    "        mean_val = df_numeric.agg(F.mean(col)).collect()[0][0]\n",
    "        if mean_val is not None:\n",
    "            means[col] = float(mean_val)\n",
    "        else:\n",
    "            means[col] = 0.0\n",
    "    \n",
    "    # Fill missing values\n",
    "    df_clean = df_numeric.fillna(means)\n",
    "    \n",
    "    # 4. Feature variance analysis (remove low-variance features)\n",
    "    print(\"🔧 Analyzing feature variance...\")\n",
    "    \n",
    "    variances = {}\n",
    "    for col in numeric_columns:\n",
    "        var_val = df_clean.agg(F.variance(col)).collect()[0][0]\n",
    "        if var_val is not None:\n",
    "            variances[col] = float(var_val)\n",
    "        else:\n",
    "            variances[col] = 0.0\n",
    "    \n",
    "    # Remove features with very low variance (threshold: 1e-6)\n",
    "    low_variance_threshold = 1e-6\n",
    "    high_variance_features = [col for col, var in variances.items() if var > low_variance_threshold]\n",
    "    \n",
    "    print(f\"🔧 Removed {len(numeric_columns) - len(high_variance_features)} low-variance features\")\n",
    "    print(f\"📊 Final feature count: {len(high_variance_features)}\")\n",
    "    \n",
    "    # 5. Select final features\n",
    "    df_selected = df_clean.select(high_variance_features)\n",
    "    \n",
    "    # 6. Assemble features into vector column\n",
    "    print(\"🔧 Assembling feature vectors...\")\n",
    "    \n",
    "    vector_assembler = VectorAssembler(\n",
    "        inputCols=high_variance_features,\n",
    "        outputCol=\"features_raw\"\n",
    "    )\n",
    "    \n",
    "    df_assembled = vector_assembler.transform(df_selected)\n",
    "    \n",
    "    # 7. Scale features\n",
    "    print(\"🔧 Scaling features...\")\n",
    "    \n",
    "    scaler = SparkStandardScaler(\n",
    "        inputCol=\"features_raw\",\n",
    "        outputCol=\"features_scaled\",\n",
    "        withStd=True,\n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    scaler_model = scaler.fit(df_assembled)\n",
    "    df_scaled = scaler_model.transform(df_assembled)\n",
    "    \n",
    "    # 8. Final selection\n",
    "    final_df = df_scaled.select(\"features_scaled\").withColumnRenamed(\"features_scaled\", \"features\")\n",
    "    \n",
    "    # Cache the result\n",
    "    final_df.cache()\n",
    "    final_row_count = final_df.count()  # Trigger caching\n",
    "    \n",
    "    print(f\"✅ Preprocessing completed!\")\n",
    "    print(f\"📊 Final dataset: {final_row_count:,} rows × {len(high_variance_features)} features\")\n",
    "    \n",
    "    return final_df, high_variance_features, scaler_model\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "start_time = time.time()\n",
    "processed_df, feature_names, scaler_model = preprocess_for_pca(spark_df)\n",
    "preprocessing_time = time.time() - start_time\n",
    "\n",
    "print(f\"⏱️  Preprocessing time: {preprocessing_time:.2f} seconds\")\n",
    "print(f\"🎯 Ready for PCA with {len(feature_names)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distributed PCA Implementation\n",
    "\n",
    "Now we'll implement PCA using Spark's MLlib for distributed linear algebra operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_pca_analysis(spark_df, n_components=None, explained_variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Perform distributed PCA using Spark MLlib\n",
    "    \n",
    "    Parameters:\n",
    "    - spark_df: Preprocessed Spark DataFrame with 'features' column\n",
    "    - n_components: Number of components (None for auto-selection)\n",
    "    - explained_variance_threshold: Threshold for automatic component selection\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing PCA results and analysis\n",
    "    \"\"\"\n",
    "    print(\"🔄 Starting distributed PCA analysis...\")\n",
    "    \n",
    "    # Convert Spark DataFrame to RDD of MLlib Vectors\n",
    "    print(\"🔧 Converting to RDD format for MLlib...\")\n",
    "    \n",
    "    def extract_features(row):\n",
    "        return MLLibVectors.dense(row.features.toArray())\n",
    "    \n",
    "    features_rdd = spark_df.rdd.map(extract_features)\n",
    "    \n",
    "    # Create RowMatrix for distributed linear algebra\n",
    "    print(\"🔧 Creating RowMatrix for distributed operations...\")\n",
    "    row_matrix = RowMatrix(features_rdd)\n",
    "    \n",
    "    print(f\"📊 Matrix dimensions: {row_matrix.numRows()} × {row_matrix.numCols()}\")\n",
    "    \n",
    "    # Compute SVD (Singular Value Decomposition)\n",
    "    print(\"🔧 Computing SVD (this may take several minutes for large datasets)...\")\n",
    "    \n",
    "    start_svd = time.time()\n",
    "    \n",
    "    # For large datasets, we limit the number of components to compute\n",
    "    max_components = min(row_matrix.numCols(), 100)  # Limit to 100 components for demo\n",
    "    \n",
    "    if n_components is None:\n",
    "        n_components = max_components\n",
    "    else:\n",
    "        n_components = min(n_components, max_components)\n",
    "    \n",
    "    # Compute SVD\n",
    "    svd = row_matrix.computeSVD(n_components, computeU=True)\n",
    "    \n",
    "    svd_time = time.time() - start_svd\n",
    "    print(f\"⏱️  SVD computation time: {svd_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract components\n",
    "    U = svd.U  # Left singular vectors (not needed for PCA)\n",
    "    s = svd.s  # Singular values\n",
    "    V = svd.V  # Right singular vectors (principal components)\n",
    "    \n",
    "    # Convert singular values to numpy for analysis\n",
    "    singular_values = np.array(s)\n",
    "    \n",
    "    # Compute explained variance\n",
    "    print(\"📊 Computing explained variance...\")\n",
    "    \n",
    "    # Variance is proportional to squared singular values\n",
    "    explained_variance = (singular_values ** 2) / (row_matrix.numRows() - 1)\n",
    "    total_variance = np.sum(explained_variance)\n",
    "    explained_variance_ratio = explained_variance / total_variance\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # Find optimal number of components\n",
    "    optimal_components = np.argmax(cumulative_variance_ratio >= explained_variance_threshold) + 1\n",
    "    \n",
    "    # Extract principal components matrix\n",
    "    components_matrix = V.toArray().T  # Transpose to get components as rows\n",
    "    \n",
    "    print(f\"✅ PCA Analysis completed!\")\n",
    "    print(f\"📊 Components computed: {len(singular_values)}\")\n",
    "    print(f\"📊 Optimal components (for {explained_variance_threshold*100}% variance): {optimal_components}\")\n",
    "    print(f\"📊 Variance explained by first component: {explained_variance_ratio[0]*100:.2f}%\")\n",
    "    print(f\"📊 Cumulative variance (first 5 components): {cumulative_variance_ratio[4]*100:.2f}%\")\n",
    "    \n",
    "    # Package results\n",
    "    results = {\n",
    "        'n_components': len(singular_values),\n",
    "        'optimal_components': optimal_components,\n",
    "        'singular_values': singular_values,\n",
    "        'explained_variance': explained_variance,\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_variance_ratio': cumulative_variance_ratio,\n",
    "        'components': components_matrix,\n",
    "        'feature_names': feature_names,\n",
    "        'svd_time': svd_time,\n",
    "        'total_variance': total_variance\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform distributed PCA\n",
    "print(\"🚀 Starting distributed PCA computation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pca_results = distributed_pca_analysis(\n",
    "    processed_df, \n",
    "    n_components=50,  # Limit for demonstration\n",
    "    explained_variance_threshold=0.95\n",
    ")\n",
    "\n",
    "total_pca_time = time.time() - start_time\n",
    "print(f\"⏱️  Total PCA analysis time: {total_pca_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA Results Validation and Comparison\n",
    "\n",
    "Let's validate our distributed PCA results by comparing with scikit-learn's PCA on a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pca_results(pca_results, processed_df, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Validate distributed PCA results against scikit-learn PCA\n",
    "    \n",
    "    Parameters:\n",
    "    - pca_results: Results from distributed PCA\n",
    "    - processed_df: Preprocessed Spark DataFrame\n",
    "    - sample_size: Size of sample for scikit-learn comparison\n",
    "    \n",
    "    Returns:\n",
    "    - Validation metrics and comparison\n",
    "    \"\"\"\n",
    "    print(\"🔄 Validating PCA results against scikit-learn...\")\n",
    "    \n",
    "    # Sample data for scikit-learn comparison\n",
    "    print(f\"📊 Sampling {sample_size} rows for validation...\")\n",
    "    \n",
    "    sample_df = processed_df.sample(False, sample_size / processed_df.count()).limit(sample_size)\n",
    "    \n",
    "    # Convert to pandas for scikit-learn\n",
    "    sample_features = np.array([row.features.toArray() for row in sample_df.collect()])\n",
    "    \n",
    "    print(f\"📊 Sample shape: {sample_features.shape}\")\n",
    "    \n",
    "    # Apply scikit-learn PCA\n",
    "    print(\"🔧 Running scikit-learn PCA for comparison...\")\n",
    "    \n",
    "    start_sklearn = time.time()\n",
    "    sklearn_pca = PCA(n_components=min(pca_results['n_components'], sample_features.shape[1]))\n",
    "    sklearn_pca.fit(sample_features)\n",
    "    sklearn_time = time.time() - start_sklearn\n",
    "    \n",
    "    print(f\"⏱️  Scikit-learn PCA time: {sklearn_time:.2f} seconds\")\n",
    "    \n",
    "    # Compare explained variance ratios\n",
    "    spark_explained_var = pca_results['explained_variance_ratio'][:len(sklearn_pca.explained_variance_ratio_)]\n",
    "    sklearn_explained_var = sklearn_pca.explained_variance_ratio_\n",
    "    \n",
    "    # Calculate comparison metrics\n",
    "    variance_diff = np.abs(spark_explained_var - sklearn_explained_var)\n",
    "    mean_variance_diff = np.mean(variance_diff)\n",
    "    max_variance_diff = np.max(variance_diff)\n",
    "    \n",
    "    print(\"📊 Validation Results:\")\n",
    "    print(f\"  Mean explained variance difference: {mean_variance_diff:.6f}\")\n",
    "    print(f\"  Max explained variance difference: {max_variance_diff:.6f}\")\n",
    "    print(f\"  Correlation of explained variances: {np.corrcoef(spark_explained_var, sklearn_explained_var)[0,1]:.6f}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"⚡ Performance Comparison:\")\n",
    "    print(f\"  Distributed PCA time: {pca_results['svd_time']:.2f} seconds\")\n",
    "    print(f\"  Scikit-learn PCA time: {sklearn_time:.2f} seconds\")\n",
    "    print(f\"  Data size ratio: {processed_df.count() / sample_size:.1f}x\")\n",
    "    \n",
    "    validation_results = {\n",
    "        'mean_variance_diff': mean_variance_diff,\n",
    "        'max_variance_diff': max_variance_diff,\n",
    "        'variance_correlation': np.corrcoef(spark_explained_var, sklearn_explained_var)[0,1],\n",
    "        'sklearn_explained_variance': sklearn_explained_var,\n",
    "        'spark_explained_variance': spark_explained_var,\n",
    "        'sklearn_time': sklearn_time,\n",
    "        'spark_time': pca_results['svd_time'],\n",
    "        'sample_size': sample_size\n",
    "    }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate results\n",
    "validation_results = validate_pca_results(pca_results, processed_df, sample_size=5000)\n",
    "\n",
    "if validation_results['mean_variance_diff'] < 0.01:\n",
    "    print(\"✅ Validation PASSED: Results are consistent with scikit-learn\")\n",
    "else:\n",
    "    print(\"⚠️  Validation WARNING: Some differences detected (expected due to sampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization and Interpretation\n",
    "\n",
    "Let's create comprehensive visualizations to understand our PCA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca_visualizations(pca_results, validation_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive PCA visualizations\n",
    "    \n",
    "    Parameters:\n",
    "    - pca_results: Results from distributed PCA\n",
    "    - validation_results: Validation comparison results\n",
    "    \"\"\"\n",
    "    print(\"🎨 Creating PCA visualizations...\")\n",
    "    \n",
    "    # Set up the plotting environment\n",
    "    plt.rcParams['figure.figsize'] = [15, 12]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Big Data PCA Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Explained Variance Ratio\n",
    "    ax1 = axes[0, 0]\n",
    "    components_range = range(1, len(pca_results['explained_variance_ratio']) + 1)\n",
    "    ax1.bar(components_range[:20], pca_results['explained_variance_ratio'][:20], \n",
    "            color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "    ax1.set_xlabel('Principal Component')\n",
    "    ax1.set_ylabel('Explained Variance Ratio')\n",
    "    ax1.set_title('Explained Variance by Component\\n(First 20 Components)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Cumulative Explained Variance\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(components_range, pca_results['cumulative_variance_ratio'], \n",
    "             'bo-', linewidth=2, markersize=4)\n",
    "    ax2.axhline(y=0.95, color='red', linestyle='--', \n",
    "                label=f'95% Threshold (Component {pca_results[\"optimal_components\"]})')\n",
    "    ax2.axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% Threshold')\n",
    "    ax2.set_xlabel('Number of Components')\n",
    "    ax2.set_ylabel('Cumulative Explained Variance')\n",
    "    ax2.set_title('Cumulative Explained Variance')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Singular Values (Scree Plot)\n",
    "    ax3 = axes[0, 2]\n",
    "    ax3.plot(components_range, pca_results['singular_values'], 'go-', linewidth=2, markersize=4)\n",
    "    ax3.set_xlabel('Component Number')\n",
    "    ax3.set_ylabel('Singular Value')\n",
    "    ax3.set_title('Scree Plot (Singular Values)')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Feature Contributions to PC1\n",
    "    ax4 = axes[1, 0]\n",
    "    pc1_contributions = np.abs(pca_results['components'][0])\n",
    "    top_features_idx = np.argsort(pc1_contributions)[-15:]  # Top 15 features\n",
    "    top_features = [pca_results['feature_names'][i] for i in top_features_idx]\n",
    "    top_contributions = pc1_contributions[top_features_idx]\n",
    "    \n",
    "    y_pos = np.arange(len(top_features))\n",
    "    ax4.barh(y_pos, top_contributions, color='lightcoral', alpha=0.8)\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels([name[:15] + '...' if len(name) > 15 else name for name in top_features], \n",
    "                        fontsize=8)\n",
    "    ax4.set_xlabel('Absolute Contribution')\n",
    "    ax4.set_title('Top Feature Contributions to PC1')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 5. Validation Comparison\n",
    "    ax5 = axes[1, 1]\n",
    "    comparison_components = range(1, len(validation_results['spark_explained_variance']) + 1)\n",
    "    ax5.plot(comparison_components, validation_results['spark_explained_variance'], \n",
    "             'bo-', label='Distributed PCA (Spark)', linewidth=2, markersize=4)\n",
    "    ax5.plot(comparison_components, validation_results['sklearn_explained_variance'], \n",
    "             'ro-', label='Scikit-learn PCA', linewidth=2, markersize=4, alpha=0.7)\n",
    "    ax5.set_xlabel('Component Number')\n",
    "    ax5.set_ylabel('Explained Variance Ratio')\n",
    "    ax5.set_title('PCA Validation: Spark vs Scikit-learn')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Performance Metrics\n",
    "    ax6 = axes[1, 2]\n",
    "    metrics = ['Mean Var Diff', 'Max Var Diff', 'Correlation']\n",
    "    values = [validation_results['mean_variance_diff'], \n",
    "              validation_results['max_variance_diff'],\n",
    "              validation_results['variance_correlation']]\n",
    "    colors = ['green' if v < 0.01 else 'orange' if v < 0.05 else 'red' for v in values[:-1]] + ['green']\n",
    "    \n",
    "    bars = ax6.bar(metrics, values, color=colors, alpha=0.7)\n",
    "    ax6.set_ylabel('Value')\n",
    "    ax6.set_title('Validation Metrics')\n",
    "    ax6.set_ylim(0, max(max(values), 1.0))\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional summary statistics\n",
    "    print(\"\\n📊 PCA Analysis Summary:\")\n",
    "    print(f\"  Total features analyzed: {len(pca_results['feature_names'])}\")\n",
    "    print(f\"  Principal components computed: {pca_results['n_components']}\")\n",
    "    print(f\"  Components for 95% variance: {pca_results['optimal_components']}\")\n",
    "    print(f\"  Dimensionality reduction ratio: {pca_results['optimal_components'] / len(pca_results['feature_names']):.2%}\")\n",
    "    print(f\"  First component explains: {pca_results['explained_variance_ratio'][0]*100:.2f}% of variance\")\n",
    "    print(f\"  Top 5 components explain: {pca_results['cumulative_variance_ratio'][4]*100:.2f}% of variance\")\n",
    "\n",
    "# Create visualizations\n",
    "create_pca_visualizations(pca_results, validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Analysis and Scalability\n",
    "\n",
    "Let's analyze the performance characteristics and scalability of our distributed PCA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance_scalability():\n",
    "    \"\"\"\n",
    "    Analyze performance characteristics and provide scalability insights\n",
    "    \"\"\"\n",
    "    print(\"📈 Performance Analysis and Scalability Assessment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Collect timing information\n",
    "    timings = {\n",
    "        'Dataset Creation': creation_time,\n",
    "        'Data Loading to Spark': loading_time,\n",
    "        'Preprocessing Pipeline': preprocessing_time,\n",
    "        'SVD Computation': pca_results['svd_time'],\n",
    "        'Total PCA Analysis': total_pca_time\n",
    "    }\n",
    "    \n",
    "    # Dataset characteristics\n",
    "    dataset_stats = {\n",
    "        'Samples': processed_df.count(),\n",
    "        'Original Features': len(df_large.columns),\n",
    "        'Final Features': len(feature_names),\n",
    "        'Data Size (MB)': df_large.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'Spark Partitions': processed_df.rdd.getNumPartitions(),\n",
    "        'Available Cores': spark.sparkContext.defaultParallelism\n",
    "    }\n",
    "    \n",
    "    print(\"🔢 Dataset Statistics:\")\n",
    "    for key, value in dataset_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value:,}\")\n",
    "    \n",
    "    print(\"\\n⏱️  Performance Breakdown:\")\n",
    "    total_time = sum(timings.values())\n",
    "    for operation, time_taken in timings.items():\n",
    "        percentage = (time_taken / total_time) * 100\n",
    "        print(f\"  {operation}: {time_taken:.2f}s ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n🎯 Total Processing Time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate throughput metrics\n",
    "    samples_per_second = dataset_stats['Samples'] / total_time\n",
    "    features_per_second = dataset_stats['Final Features'] * dataset_stats['Samples'] / total_time\n",
    "    \n",
    "    print(f\"\\n📊 Throughput Metrics:\")\n",
    "    print(f\"  Samples processed per second: {samples_per_second:,.0f}\")\n",
    "    print(f\"  Feature-samples processed per second: {features_per_second:,.0f}\")\n",
    "    \n",
    "    # Memory efficiency analysis\n",
    "    memory_per_sample = (dataset_stats['Data Size (MB)'] * 1024 * 1024) / dataset_stats['Samples']\n",
    "    print(f\"\\n💾 Memory Efficiency:\")\n",
    "    print(f\"  Memory per sample: {memory_per_sample:.0f} bytes\")\n",
    "    print(f\"  Data distributed across: {dataset_stats['Spark Partitions']} partitions\")\n",
    "    print(f\"  Approximate partition size: {dataset_stats['Data Size (MB)'] / dataset_stats['Spark Partitions']:.2f} MB\")\n",
    "    \n",
    "    # Scalability projections\n",
    "    print(f\"\\n🚀 Scalability Projections:\")\n",
    "    \n",
    "    # Project performance for different dataset sizes\n",
    "    current_samples = dataset_stats['Samples']\n",
    "    current_features = dataset_stats['Final Features']\n",
    "    \n",
    "    projections = [\n",
    "        ('1M samples', 1_000_000, current_features),\n",
    "        ('10M samples', 10_000_000, current_features),\n",
    "        ('1M samples, 200 features', 1_000_000, 200),\n",
    "        ('10M samples, 500 features', 10_000_000, 500)\n",
    "    ]\n",
    "    \n",
    "    for scenario, proj_samples, proj_features in projections:\n",
    "        # SVD complexity is roughly O(min(n*d², d³)) where n=samples, d=features\n",
    "        complexity_ratio = (proj_samples / current_samples) * (proj_features / current_features) ** 2\n",
    "        projected_time = pca_results['svd_time'] * complexity_ratio\n",
    "        \n",
    "        print(f\"  {scenario}: ~{projected_time:.1f}s (PCA computation)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n💡 Optimization Recommendations:\")\n",
    "    \n",
    "    if pca_results['svd_time'] / total_time > 0.7:\n",
    "        print(\"  • SVD computation dominates runtime - consider incremental PCA for larger datasets\")\n",
    "    \n",
    "    if dataset_stats['Spark Partitions'] < dataset_stats['Available Cores'] * 2:\n",
    "        print(f\"  • Increase partitions to {dataset_stats['Available Cores'] * 3} for better parallelization\")\n",
    "    \n",
    "    if dataset_stats['Data Size (MB)'] / dataset_stats['Spark Partitions'] > 200:\n",
    "        print(\"  • Consider smaller partitions (target: 128MB per partition) for better load balancing\")\n",
    "    \n",
    "    print(\"  • For production: add more executor nodes to distribute computation\")\n",
    "    print(\"  • For very high dimensions (>1000): consider randomized PCA or sparse PCA\")\n",
    "    print(\"  • For streaming data: implement incremental PCA with mini-batch updates\")\n",
    "\n",
    "# Run performance analysis\n",
    "analyze_performance_scalability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced PCA Applications and Interpretations\n",
    "\n",
    "Let's explore practical applications and provide interpretations of our PCA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_principal_components(pca_results):\n",
    "    \"\"\"\n",
    "    Detailed analysis of principal components and their interpretations\n",
    "    \"\"\"\n",
    "    print(\"🔍 Principal Component Analysis and Interpretation\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Analyze first few components in detail\n",
    "    n_analyze = min(5, pca_results['n_components'])\n",
    "    \n",
    "    for i in range(n_analyze):\n",
    "        print(f\"\\n📊 Principal Component {i+1}:\")\n",
    "        print(f\"  Explained Variance: {pca_results['explained_variance_ratio'][i]*100:.2f}%\")\n",
    "        print(f\"  Cumulative Variance: {pca_results['cumulative_variance_ratio'][i]*100:.2f}%\")\n",
    "        \n",
    "        # Find top contributing features\n",
    "        component = pca_results['components'][i]\n",
    "        \n",
    "        # Get absolute contributions and sort\n",
    "        abs_contributions = np.abs(component)\n",
    "        sorted_indices = np.argsort(abs_contributions)[::-1]\n",
    "        \n",
    "        print(f\"  Top 5 Contributing Features:\")\n",
    "        for j, idx in enumerate(sorted_indices[:5]):\n",
    "            feature_name = pca_results['feature_names'][idx]\n",
    "            contribution = component[idx]\n",
    "            abs_contribution = abs_contributions[idx]\n",
    "            sign = '+' if contribution > 0 else '-'\n",
    "            print(f\"    {j+1}. {feature_name[:25]:<25} ({sign}{abs_contribution:.4f})\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(f\"\\n🎯 Overall Feature Importance Analysis:\")\n",
    "    \n",
    "    # Calculate feature importance across all components (weighted by explained variance)\n",
    "    feature_importance = np.zeros(len(pca_results['feature_names']))\n",
    "    \n",
    "    for i in range(pca_results['n_components']):\n",
    "        weight = pca_results['explained_variance_ratio'][i]\n",
    "        feature_importance += weight * np.abs(pca_results['components'][i])\n",
    "    \n",
    "    # Normalize importance scores\n",
    "    feature_importance = feature_importance / np.sum(feature_importance)\n",
    "    \n",
    "    # Get top 10 most important features\n",
    "    top_feature_indices = np.argsort(feature_importance)[::-1][:10]\n",
    "    \n",
    "    print(f\"  Top 10 Most Important Features (across all components):\")\n",
    "    for i, idx in enumerate(top_feature_indices):\n",
    "        feature_name = pca_results['feature_names'][idx]\n",
    "        importance = feature_importance[idx]\n",
    "        print(f\"    {i+1:2d}. {feature_name[:30]:<30} ({importance*100:.2f}%)\")\n",
    "    \n",
    "    # Component correlation analysis\n",
    "    print(f\"\\n🔗 Component Relationship Analysis:\")\n",
    "    \n",
    "    if pca_results['n_components'] >= 3:\n",
    "        # Calculate component correlations (should be near zero for PCA)\n",
    "        comp_correlations = np.corrcoef(pca_results['components'][:3])\n",
    "        print(f\"  Correlation between first 3 components:\")\n",
    "        for i in range(3):\n",
    "            for j in range(i+1, 3):\n",
    "                corr = comp_correlations[i, j]\n",
    "                print(f\"    PC{i+1} ↔ PC{j+1}: {corr:.6f} (should be ~0)\")\n",
    "    \n",
    "    # Dimensionality reduction recommendations\n",
    "    print(f\"\\n📉 Dimensionality Reduction Recommendations:\")\n",
    "    \n",
    "    variance_thresholds = [0.80, 0.90, 0.95, 0.99]\n",
    "    for threshold in variance_thresholds:\n",
    "        n_components_needed = np.argmax(pca_results['cumulative_variance_ratio'] >= threshold) + 1\n",
    "        reduction_ratio = n_components_needed / len(pca_results['feature_names'])\n",
    "        print(f\"  For {threshold*100:2.0f}% variance: {n_components_needed:3d} components \"\n",
    "              f\"({reduction_ratio*100:.1f}% of original dimensions)\")\n",
    "    \n",
    "    return feature_importance, top_feature_indices\n",
    "\n",
    "# Perform detailed analysis\n",
    "feature_importance, top_features = analyze_principal_components(pca_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Practical Applications and Use Cases\n",
    "\n",
    "Let's explore practical applications of our distributed PCA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_pca_applications(pca_results, processed_df, feature_importance):\n",
    "    \"\"\"\n",
    "    Demonstrate practical applications of PCA results\n",
    "    \"\"\"\n",
    "    print(\"🎯 Practical Applications of Distributed PCA\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # 1. Data Compression\n",
    "    print(\"\\n💾 Data Compression Analysis:\")\n",
    "    original_features = len(pca_results['feature_names'])\n",
    "    \n",
    "    for variance_threshold in [0.90, 0.95, 0.99]:\n",
    "        n_components = np.argmax(pca_results['cumulative_variance_ratio'] >= variance_threshold) + 1\n",
    "        compression_ratio = n_components / original_features\n",
    "        storage_reduction = (1 - compression_ratio) * 100\n",
    "        \n",
    "        print(f\"  {variance_threshold*100:.0f}% variance retention:\")\n",
    "        print(f\"    Components needed: {n_components}/{original_features}\")\n",
    "        print(f\"    Storage reduction: {storage_reduction:.1f}%\")\n",
    "        print(f\"    Information loss: {(1-variance_threshold)*100:.1f}%\")\n",
    "    \n",
    "    # 2. Noise Reduction\n",
    "    print(f\"\\n🔇 Noise Reduction Capabilities:\")\n",
    "    \n",
    "    # Components with low explained variance likely represent noise\n",
    "    noise_threshold = 0.01  # Components explaining less than 1% variance\n",
    "    noise_components = np.sum(pca_results['explained_variance_ratio'] < noise_threshold)\n",
    "    signal_components = pca_results['n_components'] - noise_components\n",
    "    \n",
    "    print(f\"  Signal components (≥1% variance): {signal_components}\")\n",
    "    print(f\"  Noise components (<1% variance): {noise_components}\")\n",
    "    print(f\"  Signal-to-noise ratio: {signal_components / max(noise_components, 1):.2f}\")\n",
    "    \n",
    "    # 3. Feature Selection Insights\n",
    "    print(f\"\\n🎯 Feature Selection Insights:\")\n",
    "    \n",
    "    # Identify redundant features (low importance)\n",
    "    importance_threshold = np.mean(feature_importance)\n",
    "    important_features = feature_importance > importance_threshold\n",
    "    redundant_features = ~important_features\n",
    "    \n",
    "    print(f\"  Important features (above average): {np.sum(important_features)}\")\n",
    "    print(f\"  Potentially redundant features: {np.sum(redundant_features)}\")\n",
    "    print(f\"  Feature reduction potential: {np.sum(redundant_features)/len(feature_importance)*100:.1f}%\")\n",
    "    \n",
    "    # 4. Anomaly Detection Potential\n",
    "    print(f\"\\n🚨 Anomaly Detection Applications:\")\n",
    "    \n",
    "    # In PCA, anomalies often have large reconstruction errors\n",
    "    # For demonstration, we'll show how this could be implemented\n",
    "    \n",
    "    print(f\"  Using {pca_results['optimal_components']} components for reconstruction:\")\n",
    "    print(f\"    Captures {pca_results['cumulative_variance_ratio'][pca_results['optimal_components']-1]*100:.1f}% of variance\")\n",
    "    print(f\"    Reconstruction error threshold could be set based on remaining variance\")\n",
    "    print(f\"    Samples with high reconstruction error → potential anomalies\")\n",
    "    \n",
    "    # 5. Visualization Recommendations\n",
    "    print(f\"\\n📊 Data Visualization Recommendations:\")\n",
    "    \n",
    "    if pca_results['explained_variance_ratio'][0] > 0.5:\n",
    "        print(f\"  • First component explains {pca_results['explained_variance_ratio'][0]*100:.1f}% - good for 1D visualization\")\n",
    "    \n",
    "    two_comp_variance = pca_results['cumulative_variance_ratio'][1]\n",
    "    if two_comp_variance > 0.6:\n",
    "        print(f\"  • First 2 components explain {two_comp_variance*100:.1f}% - excellent for 2D scatter plots\")\n",
    "    \n",
    "    three_comp_variance = pca_results['cumulative_variance_ratio'][2] if pca_results['n_components'] > 2 else 0\n",
    "    if three_comp_variance > 0.7:\n",
    "        print(f\"  • First 3 components explain {three_comp_variance*100:.1f}% - good for 3D visualization\")\n",
    "    \n",
    "    # 6. Machine Learning Pipeline Integration\n",
    "    print(f\"\\n🤖 ML Pipeline Integration:\")\n",
    "    print(f\"  • Use {pca_results['optimal_components']} components as features for downstream ML models\")\n",
    "    print(f\"  • Expected speedup in ML training: {original_features / pca_results['optimal_components']:.1f}x\")\n",
    "    print(f\"  • Memory reduction in ML models: {(1 - pca_results['optimal_components']/original_features)*100:.1f}%\")\n",
    "    print(f\"  • Reduced overfitting risk due to lower dimensionality\")\n",
    "    \n",
    "    # 7. Real-world Scenario Applications\n",
    "    print(f\"\\n🌍 Real-world Application Scenarios:\")\n",
    "    print(f\"  📈 Financial Data: Reduce 100s of market indicators to key factors\")\n",
    "    print(f\"  🏥 Healthcare: Compress high-dimensional patient data for analysis\")\n",
    "    print(f\"  🛒 E-commerce: Reduce customer behavior features for recommendation systems\")\n",
    "    print(f\"  📱 IoT Sensors: Compress multi-sensor time series data\")\n",
    "    print(f\"  🔬 Genomics: Reduce thousands of gene expression features\")\n",
    "    print(f\"  📸 Computer Vision: Preprocess high-dimensional image features\")\n",
    "\n",
    "# Demonstrate applications\n",
    "demonstrate_pca_applications(pca_results, processed_df, feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Future Improvements\n",
    "\n",
    "Let's summarize our findings and discuss potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conclusions_and_recommendations():\n",
    "    \"\"\"\n",
    "    Generate comprehensive conclusions and recommendations\n",
    "    \"\"\"\n",
    "    print(\"📝 Project Conclusions and Recommendations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Key achievements\n",
    "    print(\"\\n🏆 Key Achievements:\")\n",
    "    print(f\"  ✅ Successfully implemented distributed PCA using Apache Spark\")\n",
    "    print(f\"  ✅ Processed {processed_df.count():,} samples with {len(feature_names)} features\")\n",
    "    print(f\"  ✅ Achieved {pca_results['cumulative_variance_ratio'][pca_results['optimal_components']-1]*100:.1f}% variance capture with {pca_results['optimal_components']} components\")\n",
    "    print(f\"  ✅ Validated results against scikit-learn with high accuracy\")\n",
    "    print(f\"  ✅ Demonstrated scalability for big data scenarios\")\n",
    "    \n",
    "    # Technical insights\n",
    "    print(f\"\\n🔬 Technical Insights:\")\n",
    "    print(f\"  📊 First component captured {pca_results['explained_variance_ratio'][0]*100:.1f}% of total variance\")\n",
    "    print(f\"  📊 Achieved {(1 - pca_results['optimal_components']/len(feature_names))*100:.1f}% dimensionality reduction\")\n",
    "    print(f\"  📊 SVD computation time: {pca_results['svd_time']:.2f}s for {len(feature_names)} dimensions\")\n",
    "    print(f\"  📊 Memory-efficient processing through Spark's distributed architecture\")\n",
    "    \n",
    "    # Architecture benefits\n",
    "    print(f\"\\n🏗️ Architecture Benefits Demonstrated:\")\n",
    "    print(f\"  🚀 Horizontal scalability through data partitioning\")\n",
    "    print(f\"  💾 Memory efficiency via distributed computation\")\n",
    "    print(f\"  ⚡ Parallel processing across multiple cores\")\n",
    "    print(f\"  🔄 Fault tolerance through Spark's RDD lineage\")\n",
    "    \n",
    "    # Challenges addressed\n",
    "    print(f\"\\n💪 Challenges Successfully Addressed:\")\n",
    "    print(f\"  ⚖️ Memory constraints: Distributed matrix operations\")\n",
    "    print(f\"  📈 Computational complexity: Parallel SVD computation\")\n",
    "    print(f\"  🔀 Data shuffling: Optimized partitioning strategy\")\n",
    "    print(f\"  🎯 High dimensionality: Efficient feature selection and scaling\")\n",
    "    \n",
    "    # Future improvements\n",
    "    print(f\"\\n🚀 Future Improvements and Extensions:\")\n",
    "    \n",
    "    print(f\"  🔧 Technical Enhancements:\")\n",
    "    print(f\"    • Implement Incremental PCA for streaming data\")\n",
    "    print(f\"    • Add Randomized PCA for very high-dimensional data (>10K features)\")\n",
    "    print(f\"    • Implement Sparse PCA for datasets with many zero values\")\n",
    "    print(f\"    • Add Kernel PCA for non-linear dimensionality reduction\")\n",
    "    \n",
    "    print(f\"  ⚡ Performance Optimizations:\")\n",
    "    print(f\"    • Implement approximate SVD algorithms for faster computation\")\n",
    "    print(f\"    • Add GPU acceleration for matrix operations\")\n",
    "    print(f\"    • Optimize data serialization and caching strategies\")\n",
    "    print(f\"    • Implement adaptive partitioning based on data characteristics\")\n",
    "    \n",
    "    print(f\"  🔍 Advanced Analytics:\")\n",
    "    print(f\"    • Add automated feature importance ranking\")\n",
    "    print(f\"    • Implement PCA-based anomaly detection\")\n",
    "    print(f\"    • Add component interpretation and visualization tools\")\n",
    "    print(f\"    • Integrate with MLlib pipelines for end-to-end ML workflows\")\n",
    "    \n",
    "    print(f\"  🌐 Production Considerations:\")\n",
    "    print(f\"    • Add model serialization and deployment capabilities\")\n",
    "    print(f\"    • Implement real-time PCA updates for streaming data\")\n",
    "    print(f\"    • Add comprehensive monitoring and logging\")\n",
    "    print(f\"    • Create RESTful API for PCA-as-a-Service\")\n",
    "    \n",
    "    # Best practices learned\n",
    "    print(f\"\\n📚 Best Practices Learned:\")\n",
    "    print(f\"  1. Always scale features before PCA to avoid bias toward high-variance features\")\n",
    "    print(f\"  2. Choose optimal number of partitions based on data size and cluster resources\")\n",
    "    print(f\"  3. Cache intermediate results for operations that will be reused\")\n",
    "    print(f\"  4. Monitor memory usage and adjust Spark configuration accordingly\")\n",
    "    print(f\"  5. Validate distributed results against known implementations\")\n",
    "    print(f\"  6. Consider computational complexity when choosing number of components\")\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\n🎯 Final Recommendations:\")\n",
    "    print(f\"  For datasets with:\")\n",
    "    print(f\"    • <100K samples: Use scikit-learn PCA (faster setup)\")\n",
    "    print(f\"    • 100K-10M samples: Use Spark PCA (demonstrated approach)\")\n",
    "    print(f\"    • >10M samples: Use Spark with incremental/randomized PCA\")\n",
    "    print(f\"    • >1000 features: Consider feature pre-selection or sparse methods\")\n",
    "    print(f\"    • Streaming data: Implement online/incremental PCA\")\n",
    "    \n",
    "    print(f\"\\n✨ This implementation provides a solid foundation for big data PCA\")\n",
    "    print(f\"   and can be extended for various production scenarios!\")\n",
    "\n",
    "# Generate final conclusions\n",
    "generate_conclusions_and_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup and Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark resources\n",
    "print(\"🧹 Cleaning up Spark resources...\")\n",
    "\n",
    "# Unpersist cached DataFrames\n",
    "try:\n",
    "    spark_df.unpersist()\n",
    "    processed_df.unpersist()\n",
    "    print(\"✅ Cached DataFrames unpersisted\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"✅ Spark session stopped\")\n",
    "\n",
    "print(\"\\n🎉 Big Data PCA Analysis Complete!\")\n",
    "print(\"📋 Summary of deliverables:\")\n",
    "print(\"  • Distributed PCA implementation using Apache Spark\")\n",
    "print(\"  • Performance analysis and scalability assessment\")\n",
    "print(\"  • Validation against standard library implementations\")\n",
    "print(\"  • Comprehensive visualizations and interpretations\")\n",
    "print(\"  • Practical applications and use case demonstrations\")\n",
    "print(\"  • Future improvements and production recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}